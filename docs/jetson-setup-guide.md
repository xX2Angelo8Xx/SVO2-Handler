# Jetson Orin Nano Setup Guide

**Target Hardware**: NVIDIA Jetson Orin Nano Super  
**JetPack Version**: 6.0 (L4T R36.4.4)  
**Last Updated**: December 4, 2025  
**‚ö†Ô∏è WARNING**: Training on Jetson NOT recommended - see [Training Feasibility](#training-feasibility) below

---

## Table of Contents

1. [Overview](#overview)
2. [Training Feasibility](#training-feasibility) ‚ö†Ô∏è **READ THIS FIRST**
3. [System Requirements](#system-requirements)
4. [PyTorch Installation](#pytorch-installation)
5. [cuDNN Installation](#cudnn-installation)
6. [Verification](#verification)
7. [Troubleshooting](#troubleshooting)
8. [Performance Benchmarks](#performance-benchmarks)

---

## Overview

This guide provides step-by-step instructions for setting up PyTorch with CUDA support on Jetson Orin Nano Super. Originally intended for YOLO training, field testing revealed significant limitations.

**‚ö†Ô∏è IMPORTANT**: After extensive testing, **YOLO training on Jetson Orin Nano 8GB is NOT feasible** due to memory constraints. This guide is preserved for:
- SVO2 extraction and preprocessing (works great)
- Annotation workflows (works great)
- **Future inference deployment** (after training on PC)

For YOLO training, use a PC with dedicated GPU (RTX 2060 or better).

---

## Training Feasibility

### ‚ùå Why Training Failed on Jetson

**Tested Configuration**:
- Dataset: 612 training images, 168 validation images
- Model: YOLOv8n (3M parameters)
- Batch size: 16 ‚Üí 8 ‚Üí 4 (all failed)
- Image size: 640x640

**Memory Breakdown** (8GB total):
- Base OS + JetPack: ~2GB
- ZED SDK cached: ~1GB
- PyTorch + model: ~1.5GB
- Dataset caching: ~1-2GB
- Training buffers: ~2-3GB
- **Total required**: >8GB ‚Üí **OOM kill during first epoch**

**Issues Encountered**:
1. **Out of Memory kills**: Process killed during dataset initialization
2. **cuDNN incompatibility**: PyTorch 2.3 (cuDNN 8) vs JetPack 6.0 (cuDNN 9)
3. **Complex installation**: Non-standard PyTorch wheels from NVIDIA
4. **Performance**: Only 1024 CUDA cores (vs 1920 on RTX 2060)

### ‚úÖ Recommended Workflow

**Use Jetson For** (works great):
- SVO2 file extraction from ZED camera
- Frame export and preprocessing
- Annotation (Viewer/Annotator app)
- Verification (Annotation Checker app)
- **Inference deployment** (with TensorRT optimization)

**Use PC For** (training):
- YOLO model training
- Model experimentation
- Hyperparameter tuning
- Standard PyTorch installation (no special wheels needed)

See [fieldtest-learnings.md](./fieldtest-learnings.md#yolo-training-attempts-on-jetson-orin-nano) for complete analysis.

---

## System Requirements

### Hardware
- **Device**: Jetson Orin Nano Super
- **RAM**: 8GB (shared with GPU)
  - **Training**: Insufficient for YOLO ‚ùå
  - **Inference**: Sufficient with TensorRT ‚úÖ
  - **Extraction/Annotation**: Sufficient ‚úÖ
- **Storage**: 64GB+ (for datasets and models)
- **Power**: 15W power supply recommended

### Software Prerequisites
- **JetPack**: 6.0 or later (L4T R36.x)
- **Python**: 3.10.x
- **CUDA**: 12.2 (included with JetPack)
- **cuDNN**: 8.9.7 or later (for PyTorch 2.3)

**Note**: PyTorch installation instructions below are preserved for reference and future inference deployment.

### Check Your System

```bash
# Check JetPack/L4T version
cat /etc/nv_tegra_release
# Expected output: R36 (release), REVISION: 4.4, ...

# Check Python version
python --version
# Expected: Python 3.10.12

# Check CUDA version
nvcc --version
# Expected: release 12.2

# Check existing cuDNN
dpkg -l | grep cudnn
# May show cuDNN 9.x (we'll add cuDNN 8)
```

---

## PyTorch Installation

### Step 1: Remove CPU-Only PyTorch (If Installed)

```bash
# Check current installation
pip list | grep torch

# If you see torch without CUDA support, uninstall:
pip uninstall -y torch torchvision torchaudio
```

### Step 2: Download NVIDIA PyTorch Wheels

**PyTorch 2.3 for JetPack 6.0 + Python 3.10:**

```bash
# Download PyTorch 2.3
wget https://nvidia.box.com/shared/static/mp164asf3sceb570wvjsrezk1p4ftj8t.whl \
  -O torch-2.3.0-cp310-cp310-linux_aarch64.whl

# Download torchvision 0.18
wget https://nvidia.box.com/shared/static/xpr06qe6ql3l6rj22cu3c45tz1wzi36p.whl \
  -O torchvision-0.18.0-cp310-cp310-linux_aarch64.whl
```

**File Sizes:**
- `torch-2.3.0`: ~202 MB
- `torchvision-0.18.0`: ~1.4 MB

### Step 3: Install PyTorch

```bash
pip install torch-2.3.0-cp310-cp310-linux_aarch64.whl \
            torchvision-0.18.0-cp310-cp310-linux_aarch64.whl
```

### Step 4: Verify Installation (Will Fail Without cuDNN 8)

```bash
python -c "import torch; print(torch.__version__)"
```

**Expected Error** (if only cuDNN 9 is installed):
```
ImportError: /lib/aarch64-linux-gnu/libcudnn.so.8: version `libcudnn.so.8' not found
```

This is normal! PyTorch 2.3 requires cuDNN 8. Proceed to next section.

---

## cuDNN Installation

### Background

**Why cuDNN 8?**
- PyTorch 2.3 (latest stable for Jetson) was compiled against cuDNN 8.x
- JetPack 6.0 ships with cuDNN 9.x by default
- Both versions can coexist safely

### Step 1: Download cuDNN 8.9.7

```bash
cd /home/$USER/Projects/SVO2-Handler  # Or your project directory

# Download cuDNN 8.9.7 for CUDA 12 (ARM64/sbsa)
wget https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-sbsa/cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz

# File size: ~823 MB (this will take 3-5 minutes)
```

### Step 2: Extract Archive

```bash
tar -xf cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz
```

### Step 3: Install cuDNN 8 Libraries

```bash
# Copy libraries to CUDA directory
sudo cp cudnn-linux-sbsa-8.9.7.29_cuda12-archive/lib/libcudnn* /usr/local/cuda/lib64/

# Set proper permissions
sudo chmod a+r /usr/local/cuda/lib64/libcudnn*

# Update library cache
sudo ldconfig
```

**Expected Warnings** (safe to ignore):
```
/sbin/ldconfig.real: /usr/local/cuda/.../libcudnn*.so.8 is not a symbolic link
```

These are informational only - the libraries are still usable.

### Step 4: Copy Headers (Optional, for Development)

```bash
sudo cp cudnn-linux-sbsa-8.9.7.29_cuda12-archive/include/cudnn*.h /usr/local/cuda/include/
sudo chmod a+r /usr/local/cuda/include/cudnn*
```

---

## Verification

### Comprehensive CUDA Test

```bash
python -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA version: {torch.version.cuda}')
print(f'cuDNN version: {torch.backends.cudnn.version()}')
print(f'Device count: {torch.cuda.device_count()}')
print(f'Device name: {torch.cuda.get_device_name(0)}')
"
```

**Expected Output:**
```
PyTorch version: 2.3.0
CUDA available: True
CUDA version: 12.2
cuDNN version: 8907
Device count: 1
Device name: Orin
```

### Performance Benchmark

```bash
python -c "
import torch
import time

device = torch.device('cuda')
x = torch.randn(1000, 1000, device=device)

start = time.time()
for _ in range(100):
    y = x @ x
torch.cuda.synchronize()
elapsed = time.time() - start

print(f'CUDA Performance: {elapsed:.3f}s for 100 matrix ops')
print(f'Speed: {100/elapsed:.1f} ops/sec')
print(f'GPU Memory: {torch.cuda.memory_allocated()/1024**2:.1f} MB')
"
```

**Expected Output:**
```
CUDA Performance: ~0.4-0.5s for 100 matrix ops
Speed: ~200-250 ops/sec
GPU Memory: ~16 MB
```

### Test YOLO Training Dependencies

```bash
python -c "
import torch
print('‚úÖ PyTorch OK')
import torchvision
print('‚úÖ torchvision OK')
import ultralytics
print('‚úÖ Ultralytics OK')
print(f'‚úÖ Ready to train with CUDA: {torch.cuda.is_available()}')
"
```

---

## Troubleshooting

### Issue 1: NumPy Version Incompatibility

**Symptom:**
```
UserWarning: A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.6
```

**Solution:**
```bash
pip install "numpy<2"
```

This downgrades NumPy to 1.26.x, which is compatible with PyTorch 2.3.

**Note**: You may see warnings about OpenCV and pyzed requiring NumPy 2.x. These are usually non-critical - test your specific workflows to ensure compatibility.

### Issue 2: CUDA Not Detected

**Symptom:**
```python
torch.cuda.is_available()  # Returns False
```

**Debug Steps:**
```bash
# Check CUDA toolkit
nvcc --version

# Check library path
ls -la /usr/local/cuda/lib64/libcudnn*

# Check environment variables
echo $LD_LIBRARY_PATH
echo $CUDA_HOME

# Try with explicit path
LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH python -c "import torch; print(torch.cuda.is_available())"
```

**Permanent Fix** (add to `~/.bashrc`):
```bash
export CUDA_HOME=/usr/local/cuda
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export PATH=/usr/local/cuda/bin:$PATH
```

Then reload:
```bash
source ~/.bashrc
```

### Issue 3: cuDNN 8 Not Found After Installation

**Symptom:**
```
ImportError: libcudnn.so.8: cannot open shared object file
```

**Solution:**
```bash
# Check if libraries exist
ls -la /usr/local/cuda/lib64/libcudnn*

# If missing, re-copy from archive
sudo cp cudnn-linux-sbsa-8.9.7.29_cuda12-archive/lib/libcudnn* /usr/local/cuda/lib64/

# Update cache
sudo ldconfig

# Verify library is visible
ldconfig -p | grep cudnn
```

### Issue 4: cuDNN Training Errors (EXECUTION_FAILED)

**Symptom:**
```
CuDNNError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
GET was unable to find an engine to execute this computation
Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR
```

This occurs when cuDNN 8/9 version compatibility issues arise during training. The cuDNN execution plans fail even though the library loads successfully.

**Solution - Disable cuDNN Completely** (Recommended):

The training app now automatically disables cuDNN and uses PyTorch's native CUDA kernels instead. This is more reliable than trying to work around cuDNN version mismatches.

**What happens:**
- `torch.backends.cudnn.enabled = False` in `training_app.py`
- PyTorch uses its own CUDA convolution kernels
- ~10-20% slower than optimized cuDNN, but reliable
- Still much faster than CPU-only training

**Manual workaround** (if needed):
```python
import torch
torch.backends.cudnn.enabled = False  # Disable cuDNN entirely
torch.backends.cuda.matmul.allow_tf32 = True  # Keep TF32 for performance
```

**Alternative launcher script**:
```bash
python scripts/fix_cudnn_training.py
```

**Why this works:**
- cuDNN 8.9.7 (installed) and cuDNN 9.3.0 (system) have incompatible execution plans
- Symlinks work for library loading but not for runtime execution
- PyTorch's fallback CUDA kernels don't have version dependencies
- Performance: Native CUDA ~10-15 FPS vs cuDNN ~15-20 FPS at HD720 resolution

**Performance Impact**: ~10-20% slower than optimized cuDNN, but training completes reliably.

### Issue 5: Multiple Python Environments

**Symptom:**
- PyTorch works in one terminal but not another
- `pip list` shows different packages in different sessions

**Solution:**
Check which Python you're using:
```bash
which python
python -c "import sys; print(sys.prefix)"
```

If using virtual environments:
```bash
# Deactivate any active environment
deactivate

# Install to user site-packages (recommended for Jetson)
pip install --user torch-2.3.0-cp310-cp310-linux_aarch64.whl
```

### Issue 5: Out of Memory During Training

**Symptom:**
```
RuntimeError: CUDA out of memory
```

**Solutions:**
1. **Reduce batch size** (try 8 ‚Üí 4 ‚Üí 2)
2. **Use mixed precision** (AMP - already enabled by default)
3. **Reduce image size** (640 ‚Üí 512 ‚Üí 416)
4. **Close other applications** (especially browsers)
5. **Monitor memory**:
   ```bash
   watch -n 1 tegrastats
   ```

---

## Performance Benchmarks

### Training Speed Comparison

| Configuration | FPS | 100 Epochs | Notes |
|---------------|-----|------------|-------|
| **CUDA Enabled (Correct)** | **15-20** | **2-3 hours** | ‚úÖ This guide |
| CPU-only (Wrong) | 0.1 | 50+ hours | ‚ùå Default pip install |
| Desktop RTX 3060 | 40-50 | 1-1.5 hours | üîµ Reference |

### Model Performance on Jetson

**YOLOv8n @ Various Resolutions:**

| Resolution | Training FPS | Inference FPS (TensorRT) | Memory | Recommendation |
|------------|--------------|--------------------------|--------|----------------|
| 416x416 | 25-30 | 50-60 | 400MB | ‚úÖ Fast tracking |
| 512x512 | 20-25 | 35-45 | 600MB | ‚úÖ Balanced |
| 640x640 | 15-20 | 25-35 | 800MB | ‚ö†Ô∏è High accuracy |
| **1280x720 (Source)** | **15-20** | **15-20** | **1.2GB** | ‚úÖ **Long-range (30-40m)** |
| 1280x1280 | 8-12 | 10-15 | 1.8GB | ‚ùå Too slow |

**Note**: Source resolution (1280x720) recommended for 30-40m detection range to preserve small targets.

### Memory Usage Breakdown

**Typical Training Session (YOLOv8n @ 1280x720):**

| Component | Memory | Percentage |
|-----------|--------|------------|
| YOLO Model | ~500MB | 6% |
| Training Data | ~300MB | 4% |
| CUDA Context | ~200MB | 3% |
| Optimizer State | ~400MB | 5% |
| Available | ~6.6GB | 82% |
| **Total Used** | **~1.4GB** | **18%** |

**Plenty of headroom** for real-time inference + ZED depth processing.

---

## Post-Installation Checklist

- [ ] PyTorch 2.3.0 installed and imports successfully
- [ ] `torch.cuda.is_available()` returns `True`
- [ ] cuDNN version shows `8907` or `89XX`
- [ ] Device name shows `Orin`
- [ ] NumPy version is `1.26.x` (not 2.x)
- [ ] Ultralytics imports without errors
- [ ] CUDA performance test completes in <0.5s
- [ ] Training app launches without CUDA errors
- [ ] `tegrastats` shows GPU activity during training

---

## Additional Resources

### Official Documentation
- **NVIDIA PyTorch for Jetson**: https://forums.developer.nvidia.com/t/pytorch-for-jetson/72048
- **JetPack Documentation**: https://developer.nvidia.com/embedded/jetpack
- **Ultralytics YOLO**: https://docs.ultralytics.com/

### Useful Commands

```bash
# Monitor GPU usage during training
sudo tegrastats

# Check GPU memory
nvidia-smi  # Not available on Jetson, use tegrastats

# Check CUDA processes
fuser -v /dev/nvidia*

# Check disk space (datasets are large!)
df -h

# Check available memory
free -h

# Monitor temperature
cat /sys/devices/virtual/thermal/thermal_zone*/temp
```

### Power Modes

Jetson has configurable power modes. For training, use maximum performance:

```bash
# Check current mode
sudo nvpmodel -q

# Set to maximum performance (Mode 0 - 15W)
sudo nvpmodel -m 0

# Enable jetson_clocks for maximum frequency
sudo jetson_clocks

# Verify clocks
sudo jetson_clocks --show
```

**Note**: Ensure adequate cooling when running at max performance during long training sessions.

---

## Backup Your Wheels

After successful download, back up the PyTorch wheels:

```bash
# Copy to a safe location
cp torch-2.3.0-cp310-cp310-linux_aarch64.whl ~/torch_wheels_backup/
cp torchvision-0.18.0-cp310-cp310-linux_aarch64.whl ~/torch_wheels_backup/
cp cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz ~/torch_wheels_backup/

# Or add to your project repository
cp *.whl /path/to/SVO2-Handler/
```

This saves ~4GB of download time if you need to reinstall later.

---

## Summary: Quick Install Script

For future reference or other Jetson systems:

```bash
#!/bin/bash
# Quick PyTorch + CUDA setup for Jetson Orin Nano (JetPack 6.0)

set -e  # Exit on error

echo "=== Jetson PyTorch + cuDNN Setup ==="

# 1. Remove CPU-only PyTorch if present
pip uninstall -y torch torchvision 2>/dev/null || true

# 2. Download PyTorch wheels
wget -q --show-progress https://nvidia.box.com/shared/static/mp164asf3sceb570wvjsrezk1p4ftj8t.whl -O torch-2.3.0-cp310-cp310-linux_aarch64.whl
wget -q --show-progress https://nvidia.box.com/shared/static/xpr06qe6ql3l6rj22cu3c45tz1wzi36p.whl -O torchvision-0.18.0-cp310-cp310-linux_aarch64.whl

# 3. Install PyTorch
pip install torch-2.3.0-cp310-cp310-linux_aarch64.whl torchvision-0.18.0-cp310-cp310-linux_aarch64.whl

# 4. Download cuDNN 8
if [ ! -f cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz ]; then
    wget -q --show-progress https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-sbsa/cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz
fi

# 5. Extract and install cuDNN
tar -xf cudnn-linux-sbsa-8.9.7.29_cuda12-archive.tar.xz
sudo cp cudnn-linux-sbsa-8.9.7.29_cuda12-archive/lib/libcudnn* /usr/local/cuda/lib64/
sudo chmod a+r /usr/local/cuda/lib64/libcudnn*
sudo ldconfig

# 6. Fix NumPy compatibility
pip install "numpy<2"

# 7. Verify installation
echo ""
echo "=== Verification ==="
python -c "
import torch
print(f'‚úÖ PyTorch: {torch.__version__}')
print(f'‚úÖ CUDA: {torch.cuda.is_available()}')
print(f'‚úÖ cuDNN: {torch.backends.cudnn.version()}')
print(f'‚úÖ Device: {torch.cuda.get_device_name(0)}')
"

echo ""
echo "=== Setup Complete! ==="
echo "You can now train YOLO models with CUDA acceleration."
```

Save as `setup_pytorch_jetson.sh`, make executable with `chmod +x setup_pytorch_jetson.sh`, and run with `./setup_pytorch_jetson.sh`.

---

**Last Updated**: December 3, 2025  
**Verified On**: Jetson Orin Nano Super, JetPack 6.0 (L4T R36.4.4), Python 3.10.12
