# YOLO Training GUI - Comprehensive Feature Guide

**Last Updated**: December 3, 2025

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Jetson Orin Nano Super Recommendations](#jetson-orin-nano-super-recommendations)
3. [Dataset Configuration](#dataset-configuration)
4. [Model Selection](#model-selection)
5. [Training Parameters](#training-parameters)
6. [Augmentation Settings](#augmentation-settings)
7. [Performance vs. Accuracy Trade-offs](#performance-vs-accuracy-trade-offs)
8. [Real-World Deployment Considerations](#real-world-deployment-considerations)

---

## Executive Summary

This guide explains every feature in the YOLO Training GUI (`training_app.py`) with detailed background information, implications, and recommendations specifically for **real-time drone target tracking on Jetson Orin Nano Super** during high-speed operations.

**Critical Constraint**: The system must simultaneously:
- Run YOLO object detection (target localization)
- Process ZED depth maps (distance calculation)
- Execute trajectory prediction (path planning)
- Maintain real-time performance (5-10 Hz minimum for flight controller)

**Key Takeaway**: For Jetson Orin Nano Super deployment, **YOLOv8n (nano)** is the recommended model, providing the best balance of speed and accuracy for real-time tracking.

---

## Jetson Orin Nano Super Recommendations

### Hardware Specifications
- **GPU**: NVIDIA Ampere (1024 CUDA cores)
- **GPU Memory**: 8GB shared
- **AI Performance**: 67 TOPS (INT8)
- **Power Budget**: 7-15W typical
- **Thermal Constraints**: Passive cooling (40¬∞C ambient), active cooling available

### Recommended Configuration for Real-Time Tracking

```yaml
Model: YOLOv8n (nano variant)
Image Size: 416x416 or 512x512
Inference Mode: TensorRT FP16 or INT8
Expected FPS: 25-40 FPS (416), 15-25 FPS (512)
Power Draw: ~10W sustained
```

### Why YOLOv8n?

**Performance Benchmarks (Jetson Orin Nano Super)**:

| Model      | Input Size | FPS (FP16) | FPS (INT8) | mAP50 | Parameters | Model Size |
|------------|------------|------------|------------|-------|------------|------------|
| YOLOv5n    | 416        | 30-35      | 45-50      | ~45%  | 1.9M       | 3.8 MB     |
| YOLOv5n    | 640        | 15-20      | 25-30      | ~48%  | 1.9M       | 3.8 MB     |
| **YOLOv8n**| **416**    | **35-40**  | **50-60**  | **52%**| **3.2M**  | **6.2 MB** |
| **YOLOv8n**| **512**    | **25-30**  | **40-45**  | **54%**| **3.2M**  | **6.2 MB** |
| YOLOv8n    | 640        | 15-20      | 28-35      | ~55%  | 3.2M       | 6.2 MB     |
| YOLOv8s    | 416        | 20-25      | 35-40      | ~62%  | 11.2M      | 22 MB      |
| YOLOv8s    | 640        | 8-12       | 15-20      | ~65%  | 11.2M      | 22 MB      |

**Recommendation**: **YOLOv8n @ 416x416 with TensorRT INT8**
- **FPS**: 50-60 (leaves headroom for depth + trajectory)
- **Accuracy**: 52% mAP50 (sufficient for single-class tracking)
- **Latency**: ~16-20ms per frame
- **Power**: ~8-10W
- **Headroom**: 40-45 FPS budget for depth processing + tracking

**Alternative for Higher Accuracy**: **YOLOv8n @ 512x512 with TensorRT FP16**
- **FPS**: 25-30 (tighter but acceptable)
- **Accuracy**: 54% mAP50
- **Use if**: Tracking at longer distances (>30m) where higher resolution helps

**DO NOT USE**:
- ‚ùå YOLOv8s or larger (too slow, <15 FPS)
- ‚ùå 640x640 input (unless stationary/slow targets)
- ‚ùå FP32 precision (2-3x slower than FP16)

### System Resource Allocation

**Total Available Resources**:
- GPU: 8GB VRAM (shared with system)
- CPU: 6-core ARM Cortex-A78AE
- Power: 15W budget (target 10-12W sustained)

**Estimated Resource Usage**:

| Component           | VRAM    | Compute (ms) | Power (W) |
|---------------------|---------|--------------|-----------|
| YOLO (YOLOv8n@416) | ~500MB  | 16-20ms      | 3-4W      |
| ZED Depth Processing| ~1.5GB  | 10-15ms      | 2-3W      |
| Tracker (CSRT)      | ~100MB  | 5-8ms        | 0.5-1W    |
| Trajectory Calc     | ~50MB   | 2-5ms        | 0.5W      |
| System Overhead     | ~500MB  | 5-10ms       | 2-3W      |
| **Total**           | **~2.6GB** | **38-58ms** | **8-12W** |

**Resulting Performance**: 17-26 FPS end-to-end ‚Üí Sufficient for 5-10 Hz flight controller updates

---

## Resolution Strategy: Training at Source Resolution

### The Problem: Downscaling Kills Small Object Detection

**Scenario**: ZED2i HD720 (1280x720) captures at 30-40m distance
- Target appears as small blob: ~10-30 pixels across
- Standard YOLO training: Downscale to 416x416 or 640x640
- **Result**: Small targets become undetectable

**The Math**:
```
Original:     1280x720 @ 20x20 pixel target
Downscale to 416x416:
  - Width factor:  416/1280 = 0.325 (32.5%)
  - Height factor: 416/720  = 0.578 (57.8%)
  - Target becomes: 6.5x11.6 pixels (distorted + tiny)
  - YOLO minimum: 8-16 pixels for reliable detection
  - Result: Detection FAILURE at range
```

### Solution: Use Source Resolution Training

**Enable in Training GUI**: 
- Training Tab ‚Üí ‚òëÔ∏è "Use Source Resolution (train at native camera resolution, e.g., 1280x720)"
- When checked: Image size dropdown is disabled
- YOLO trains at native image dimensions (1280x720)

**Benefits**:
‚úÖ **No information loss** - Every pixel from camera used  
‚úÖ **Preserves small targets** - 20-pixel blob stays 20 pixels  
‚úÖ **Direct camera-to-model** - No resize overhead during inference  
‚úÖ **Optimal for long-range** - Detects targets at 30-40m reliably  

**Trade-offs**:
‚ö†Ô∏è **Slower inference** - 15-20 FPS vs. 50-60 FPS at 416x416  
‚ö†Ô∏è **More VRAM** - ~1.2GB vs. ~400MB  
‚ö†Ô∏è **Longer training** - ~1.5-2x training time per epoch  

### When to Use Source Resolution

| Scenario | Use Source Resolution? | Reason |
|----------|------------------------|--------|
| **Long-range tracking (30-40m)** | ‚úÖ **YES** | Small targets need every pixel |
| **VGA recording (672x376)** | ‚ùå NO | Already low-res, downscale to 416 OK |
| **Close-range only (<15m)** | ‚ùå NO | Targets large enough after downscale |
| **High-speed tracking** | ‚ö†Ô∏è MAYBE | Need 30+ FPS? Use 416. Accuracy critical? Use source. |
| **Stationary camera** | ‚úÖ YES | No speed constraint, maximize accuracy |
| **Multi-target detection** | ‚ùå NO | More targets = need speed, use 416/512 |

### ZED2i Resolution Modes vs. YOLO Training

**ZED2i Available Modes**:

| Mode | Resolution | FPS | Use Case | Train At |
|------|------------|-----|----------|----------|
| **HD720** | 1280x720 | 60 | Current setup, good range | **1280x720 (source)** |
| HD1080 | 1920x1080 | 30 | High detail, slow | 1920x1080 (source) |
| HD2K | 2208x1242 | 15 | Maximum detail | 1920x1080 or source |
| **VGA** | 672x376 | 100 | High-speed, low detail | **416x416 (downscale OK)** |

**Recommendation for Your Setup**:

**Current (HD720 @ 1280x720)**:
- ‚úÖ **Train at source resolution (1280x720)**
- Inference: 15-20 FPS on Jetson (TensorRT FP16)
- Detection range: 30-40m reliably
- VRAM: ~1.2GB
- **Best for**: Long-range tracking with acceptable FPS

**Alternative (Switch to VGA @ 672x376)**:
- ‚ùå **Train at 416x416 (downscale acceptable)**
- Inference: 50-60 FPS on Jetson (TensorRT INT8)
- Detection range: 15-25m (reduced!)
- VRAM: ~400MB
- **Best for**: High-speed close-range tracking

### Performance Comparison: HD720 Source vs. VGA Downscaled

**HD720 Source Resolution (1280x720 training + inference)**:

| Metric | Value | Notes |
|--------|-------|-------|
| Training time/epoch | 15-20 min | 2x slower than 416 |
| Inference FPS (Jetson) | 15-20 FPS | TensorRT FP16 |
| Detection range | 30-40m | Full capability |
| VRAM usage | ~1.2GB | 33% of 8GB budget |
| Power draw | ~10-12W | Within budget |
| Target size @ 35m | 20x20 pixels | Detectable |

**VGA @ 416x416 (672x376 source ‚Üí 416x416 training)**:

| Metric | Value | Notes |
|--------|-------|-------|
| Training time/epoch | 8-10 min | Baseline |
| Inference FPS (Jetson) | 50-60 FPS | TensorRT INT8 |
| Detection range | 15-25m | **REDUCED** |
| VRAM usage | ~400MB | 15% of budget |
| Power draw | ~8-10W | Excellent |
| Target size @ 35m | Too small | **NOT DETECTABLE** |

### Aspect Ratio Considerations

**HD720 (1280x720) ‚Üí Square Downscaling**:
```
Aspect ratio: 16:9 (1.78:1)
Downscale to 416x416 (1:1):
  - Horizontal compression: 1280‚Üí416 (32.5%)
  - Vertical compression:   720‚Üí416 (57.8%)
  - Distortion factor: 1.78x (significant!)
  - Impact: Circular targets become oval
```

**HD720 (1280x720) ‚Üí Source Resolution Training**:
```
Aspect ratio: 16:9 (1.78:1) PRESERVED
No distortion:
  - Width:  1280 ‚Üí 1280 (100%)
  - Height: 720 ‚Üí 720 (100%)
  - Distortion: None
  - Impact: Targets maintain shape
```

**Recommendation**: For HD720, **use source resolution** to avoid distortion + preserve small targets.

### Technical Implementation Details

**How It Works**:
1. User enables "Use Source Resolution" checkbox
2. Training GUI sets `image_size = -1` (special flag)
3. `training_config.py` checks: if `image_size < 0`, omit `imgsz` parameter
4. YOLO reads first image, determines native resolution (1280x720)
5. All images trained at this resolution (no resize)
6. During inference: Camera feed directly to model (no preprocessing)

**Requirements**:
- ‚úÖ All images must have **same resolution** (verified during export)
- ‚úÖ Images must be **rectangular** (not square)
- ‚ö†Ô∏è Higher VRAM required (~3x more than 416x416)
- ‚ö†Ô∏è Longer training time (~1.5-2x)

### Decision Matrix: HD720 Source vs. Downscale

Use this table to decide your strategy:

| Requirement | HD720 Source (1280x720) | Downscale (416x416) |
|-------------|-------------------------|---------------------|
| Max detection range | ‚úÖ 30-40m | ‚ùå 15-25m |
| Small target detection | ‚úÖ 10-20 pixels OK | ‚ùå <8 pixels fails |
| Inference speed | ‚ö†Ô∏è 15-20 FPS | ‚úÖ 50-60 FPS |
| Training time | ‚ö†Ô∏è 15-20 min/epoch | ‚úÖ 8-10 min/epoch |
| VRAM usage | ‚ö†Ô∏è 1.2GB | ‚úÖ 400MB |
| Aspect ratio | ‚úÖ 16:9 preserved | ‚ùå Forced 1:1 |
| Real-time tracking | ‚úÖ 15 FPS sufficient | ‚úÖ 50 FPS overkill |
| Flight controller | ‚úÖ 10Hz OK | ‚úÖ 10Hz OK |

**Verdict**: For your 30-40m tracking requirement with HD720 recordings, **use source resolution training**.

---

## Dataset Configuration

### Source Training Folder

**Path**: Original 73-bucket YOLO training folder  
**Location**: `/path/to/YoloTrainingV1/`

**What it is**: Your existing annotated dataset with the hierarchical bucket structure:
```
YoloTrainingV1/
‚îú‚îÄ‚îÄ 0_far/                    # target_far class (>40m, no depth)
‚îú‚îÄ‚îÄ 1_S/ ‚Üí Bot/near/          # target_close class (0-40m, with depth)
‚îú‚îÄ‚îÄ 1_S/ ‚Üí Bot/mid/
‚îú‚îÄ‚îÄ 1_S/ ‚Üí Bot/far/
‚îî‚îÄ‚îÄ ... (72 more buckets)
```

**Implications**:
- ‚úÖ **Read-Only**: Original folder is NEVER modified
- ‚úÖ **Preserves History**: All annotations remain intact
- ‚úÖ **Reusable**: Can generate multiple training datasets from same source
- ‚ö†Ô∏è **Folder Size**: May be large (10-50GB+), ensure sufficient disk space

**Best Practices**:
- Keep source folder on fast storage (SSD preferred)
- Back up regularly (external drive or cloud)
- Version control via folder names (e.g., `YoloTrainingV1`, `YoloTrainingV2`)

---

### Output Dataset Folder

**Path**: Where the YOLO-formatted dataset will be created  
**Location**: `/path/to/yolo_dataset_run1/`

**What it is**: A new folder with standardized YOLO structure:
```
yolo_dataset_run1/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/    # 70% of images (copied from source)
‚îÇ   ‚îú‚îÄ‚îÄ val/      # 20% of images
‚îÇ   ‚îî‚îÄ‚îÄ test/     # 10% of images
‚îú‚îÄ‚îÄ labels/
‚îÇ   ‚îú‚îÄ‚îÄ train/    # Corresponding .txt annotations
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îú‚îÄ‚îÄ data.yaml     # YOLO configuration file
‚îú‚îÄ‚îÄ train.txt     # List of training image paths
‚îú‚îÄ‚îÄ val.txt       # List of validation image paths
‚îî‚îÄ‚îÄ test.txt      # List of test image paths
```

**Implications**:
- üíæ **Disk Space**: Requires ~same size as source (images are copied)
- üîÑ **Regenerable**: Can delete and recreate anytime from source
- üìä **Training-Specific**: Each training run should use a unique output folder
- üöÄ **Performance**: Use SSD for faster I/O during training

**Naming Convention**:
```
yolo_dataset_<date>_<experiment>
Examples:
  yolo_dataset_2025-12-03_baseline
  yolo_dataset_2025-12-03_heavy_aug
  yolo_dataset_2025-12-04_high_res
```

**Disk Space Planning**:
- Source: 20GB ‚Üí Output: 20GB
- Multiple experiments: 20GB √ó 5 runs = 100GB
- **Recommendation**: Reserve 200-300GB for experiments

---

### Data Split Ratios

**Train Ratio** (Default: 0.7 = 70%)  
**Val Ratio** (Default: 0.2 = 20%)  
**Test Ratio** (Default: 0.1 = 10%)

#### What These Mean

**Training Set** (70%):
- Images used to train the model (backpropagation)
- Model learns patterns from these images
- Largest portion to maximize learning

**Validation Set** (20%):
- Images used during training to tune hyperparameters
- Model does NOT train on these (no backprop)
- Used to detect overfitting
- Metrics (mAP, loss) calculated each epoch

**Test Set** (10%):
- Images NEVER seen during training
- Final evaluation after training completes
- Represents real-world performance
- Optional: Set to 0% if data is scarce

#### Implications of Different Splits

| Split         | Use Case                              | Pros                          | Cons                        |
|---------------|---------------------------------------|-------------------------------|-----------------------------|
| 70/20/10      | Standard (default)                    | Balanced, industry standard   | -                           |
| 80/15/5       | Small dataset (<500 images)           | More training data            | Less reliable validation    |
| 60/30/10      | Hyperparameter tuning                 | Better validation accuracy    | Less training data          |
| 70/30/0       | Skip test set                         | More data for train/val       | No final benchmark          |
| 50/25/25      | Research/academic                     | Strong test reliability       | Less training data          |

**Recommendations**:

- **< 500 images**: Use 80/15/5 (maximize training data)
- **500-2000 images**: Use 70/20/10 (default, balanced)
- **> 2000 images**: Use 70/20/10 or 60/30/10 (validation more critical)
- **Experimenting**: Use 70/30/0 (iterate faster, skip test)
- **Production**: Use 70/20/10 (need final test benchmark)

**Your Case (Drone Tracking)**:
- Likely 500-2000+ images from field tests
- **Recommended**: 70/20/10 (default)
- **Why**: Need reliable validation to prevent overfitting on repetitive backgrounds

#### Validation vs. Test: Why Both?

**Validation Set**:
- Used DURING training (every epoch)
- Influences decisions: early stopping, best checkpoint selection
- Can cause "validation set overfitting" if used too much

**Test Set**:
- Used AFTER training (once)
- Pure performance metric
- Represents unseen real-world data

**Analogy**:
- Training = Studying from textbook
- Validation = Practice exams (tune your study method)
- Test = Final exam (never seen before)

---

### Include Negative Samples

**Checkbox**: Include negative samples (backgrounds)  
**Default**: ‚úÖ Checked (Enabled)

#### What Are Negative Samples?

Images from `negative_samples/` folder:
- Frames with NO target visible
- Examples: Clouds, ground, trees, sky, buildings
- NO `.txt` annotation file (indicates "no objects")

#### Purpose

**Problem**: Without negatives, YOLO may produce false positives:
- Detects trees as targets
- Detects clouds as targets
- Detects ground features as targets

**Solution**: Train on negative examples:
- Model learns what is NOT a target
- Reduces false positive rate
- Improves precision (fewer spurious detections)

#### Implications

| Setting      | Pros                                  | Cons                               |
|--------------|---------------------------------------|------------------------------------|
| ‚úÖ Enabled   | Lower false positives                 | Slightly longer training time      |
| ‚úÖ Enabled   | Better precision                      | Requires negative sample collection|
| ‚úÖ Enabled   | Robust to background clutter          | May reduce recall slightly         |
| ‚ùå Disabled  | Faster training                       | Higher false positive rate         |
| ‚ùå Disabled  | No negative collection needed         | Poor performance on cluttered scenes|

**Best Practices**:

1. **Collect diverse negatives**:
   - Different lighting (dawn, noon, dusk)
   - Different backgrounds (sky, ground, trees)
   - Different weather (clear, cloudy, foggy)

2. **Negative-to-positive ratio**:
   - **Target**: 10-30% of dataset should be negatives
   - **Example**: 1000 target images ‚Üí 100-300 negative images
   - **Too many**: Model becomes too conservative (low recall)
   - **Too few**: Model still has false positives

3. **When to disable**:
   - ‚ùå No negative samples collected yet
   - ‚ùå Very clean backgrounds (open sky only)
   - ‚úÖ For initial baseline training (compare with/without)

**Your Case (Drone Tracking)**:
- **Recommendation**: ‚úÖ **Enable**
- **Why**: High-speed chase ‚Üí varied backgrounds (sky, ground transitions)
- **Action**: Ensure ~15-20% of dataset is negative samples

---

### Shuffle Data Before Splitting

**Checkbox**: Shuffle data before splitting  
**Default**: ‚úÖ Checked (Enabled)

#### What It Does

**Without Shuffle**:
```
Original order: frame_001, frame_002, ..., frame_1000
Train: frame_001-700 (first 70%)
Val:   frame_701-900 (next 20%)
Test:  frame_901-1000 (last 10%)
```

**With Shuffle**:
```
Randomized order: frame_453, frame_012, frame_891, ...
Train: Random 70%
Val:   Random 20%
Test:  Random 10%
```

#### Why Shuffle?

**Problem**: Sequential frames are highly correlated:
- Frame N and Frame N+1 are nearly identical
- Same lighting, same background, same flight phase
- If train/val/test are sequential blocks ‚Üí data leakage

**Solution**: Shuffle breaks temporal correlation:
- Train/val/test have diverse samples
- Model generalizes better
- Validation metrics are more reliable

#### Implications

| Setting      | Pros                                  | Cons                               |
|--------------|---------------------------------------|------------------------------------|
| ‚úÖ Enabled   | Better generalization                 | Can't reproduce exact split        |
| ‚úÖ Enabled   | No data leakage between splits        | (Use random seed to fix this)      |
| ‚úÖ Enabled   | More robust validation metrics        | -                                  |
| ‚ùå Disabled  | Reproducible splits (if needed)       | High correlation between splits    |
| ‚ùå Disabled  | Preserves temporal sequences          | Validation metrics unreliable      |
| ‚ùå Disabled  | -                                     | Poor generalization                |

**When to Disable**:
- ‚ùå Only for video sequence analysis (rare)
- ‚ùå Time-series prediction (not applicable here)
- ‚úÖ **Always enable for object detection**

**Best Practices**:
- ‚úÖ Always shuffle for object detection
- ‚úÖ Use fixed random seed for reproducibility
- ‚úÖ Re-shuffle if adding new data

**Your Case (Drone Tracking)**:
- **Recommendation**: ‚úÖ **Always Enable**
- **Why**: Frames from same flight are correlated; shuffle ensures diversity

---

### Random Seed

**Input**: Integer (0-99999)  
**Default**: 42

#### What It Does

Controls the random number generator for:
- Data shuffling
- Train/val/test split assignment

**Same seed = Same split** (reproducible)  
**Different seed = Different split** (variation)

#### Reproducibility

**Scenario 1: Comparing Models**
```
Experiment A: YOLOv8n, seed=42
Experiment B: YOLOv8s, seed=42
‚Üí Both use SAME train/val/test split ‚Üí Fair comparison
```

**Scenario 2: Ablation Studies**
```
Baseline: No augmentation, seed=42
Test 1:   Heavy augmentation, seed=42
Test 2:   Different optimizer, seed=42
‚Üí All experiments use SAME data split ‚Üí Isolate variable
```

**Scenario 3: Variation Testing**
```
Run 1: seed=42
Run 2: seed=123
Run 3: seed=999
‚Üí Different splits ‚Üí Test model stability across data variations
```

#### Implications

| Use Case                  | Seed Strategy              | Reason                          |
|---------------------------|----------------------------|---------------------------------|
| Comparing models          | Same seed (e.g., 42)       | Fair comparison                 |
| Ablation studies          | Same seed                  | Isolate single variable         |
| Final model training      | Same seed                  | Reproducible for production     |
| Robustness testing        | Multiple different seeds   | Test stability                  |
| Publishing results        | Document seed used         | Others can reproduce            |

**Common Seeds**:
- `42`: Most common (Deep Learning community convention)
- `0`: Simple, easy to remember
- `123`: Alternative common choice
- `<date>`: e.g., `20251203` for December 3, 2025

**Best Practices**:
1. **Use same seed** for all related experiments
2. **Document seed** in experiment logs
3. **Test multiple seeds** (3-5) for final model to ensure stability
4. **Average results** across seeds for published metrics

**Your Case (Drone Tracking)**:
- **Recommendation**: Use `42` (default) for all initial experiments
- **Later**: Test with seeds `42`, `123`, `999` ‚Üí Average performance

---

## Model Selection

### YOLO Version

**Options**: YOLOv8, YOLOv5  
**Default**: YOLOv8

#### YOLOv8 (Ultralytics, 2023)

**Architecture**: Latest generation, improved backbone  
**Performance**: ~10-15% better mAP than YOLOv5  
**Speed**: Similar or faster inference  
**Training**: Faster convergence (fewer epochs needed)

**Key Improvements**:
- C2f blocks (better feature propagation)
- Anchor-free design (simpler, more robust)
- Better small object detection
- Improved loss function (DFL + BCE)

**Use Cases**:
- ‚úÖ New projects (modern architecture)
- ‚úÖ Better accuracy needed
- ‚úÖ Limited training data (converges faster)
- ‚úÖ **Recommended for production**

#### YOLOv5 (Ultralytics, 2020)

**Architecture**: Proven, mature, widely deployed  
**Performance**: Solid, but 10-15% lower mAP than v8  
**Speed**: Slightly slower than v8  
**Community**: Larger, more examples/tutorials

**Use Cases**:
- ‚úÖ Legacy compatibility
- ‚úÖ Extensive community resources needed
- ‚úÖ Known performance baseline
- ‚ö†Ô∏è Use only if specific requirement

#### Comparison Table

| Feature              | YOLOv8                | YOLOv5                |
|----------------------|-----------------------|-----------------------|
| **mAP50**            | 52% (n), 62% (s)      | 45% (n), 55% (s)      |
| **Speed (Jetson)**   | 35 FPS (n@416)        | 30 FPS (n@416)        |
| **Training Time**    | 50 epochs sufficient  | 80-100 epochs needed  |
| **Backbone**         | C2f (improved)        | C3 (older)            |
| **Anchors**          | Anchor-free ‚úÖ        | Anchor-based          |
| **Small Objects**    | Better                | Good                  |
| **Maturity**         | Newer (2023)          | Mature (2020)         |
| **Documentation**    | Good                  | Extensive             |

**Recommendation for Jetson Orin Nano**:
- **Use YOLOv8**: Better accuracy, faster inference, modern architecture
- **Avoid YOLOv5**: Only if legacy compatibility required

---

### Model Variant

**Options**: n (nano), s (small), m (medium), l (large), x (xlarge)  
**Default**: n (nano)

#### Variant Comparison

| Variant | Parameters | Model Size | Accuracy (mAP50) | Jetson FPS (416) | Use Case                  |
|---------|------------|------------|------------------|------------------|---------------------------|
| **n**   | 3.2M       | 6 MB       | 52%              | 35-40            | **Jetson deployment** ‚úÖ  |
| **s**   | 11.2M      | 22 MB      | 62%              | 20-25            | Moderate accuracy needs   |
| **m**   | 25.9M      | 52 MB      | 68%              | 10-15            | ‚ùå Too slow for Jetson   |
| **l**   | 43.7M      | 87 MB      | 71%              | 6-8              | ‚ùå Too slow               |
| **x**   | 68.2M      | 136 MB     | 73%              | 4-6              | ‚ùå Too slow               |

#### Detailed Analysis

**Nano (n)** - ‚úÖ RECOMMENDED
- **Best for**: Real-time embedded systems
- **Accuracy**: 52% mAP50 (sufficient for single-class tracking)
- **Speed**: 35-40 FPS on Jetson (excellent)
- **Memory**: 6 MB (tiny, fits easily)
- **Latency**: ~16-20ms (low)
- **Use if**: Real-time performance critical (YOUR CASE)

**Small (s)** - ‚ö†Ô∏è FALLBACK OPTION
- **Best for**: Balance between speed and accuracy
- **Accuracy**: 62% mAP50 (+10% vs nano)
- **Speed**: 20-25 FPS on Jetson (acceptable)
- **Memory**: 22 MB (still small)
- **Latency**: ~40-50ms (moderate)
- **Use if**: Tracking fails with nano (try this)

**Medium (m)** - ‚ùå NOT RECOMMENDED
- **Too slow**: 10-15 FPS (insufficient headroom)
- **Use if**: Offline processing only (not live tracking)

**Large (l), X-Large (x)** - ‚ùå NOT VIABLE
- **Way too slow**: <10 FPS
- **Use if**: Desktop-only, no real-time requirement

#### Performance vs. Accuracy Trade-off

**Accuracy Gain vs. Speed Loss**:
```
n ‚Üí s:  +10% mAP, -40% FPS  (20 FPS ‚Üí too tight)
s ‚Üí m:  +6% mAP,  -50% FPS  (10 FPS ‚Üí unusable)
m ‚Üí l:  +3% mAP,  -30% FPS  (7 FPS ‚Üí unusable)
```

**Diminishing Returns**: Larger models give less accuracy gain per speed loss.

#### Memory Considerations

**Jetson Orin Nano Super (8GB shared VRAM)**:
```
Model Nano:   ~500 MB VRAM during inference
Model Small:  ~800 MB VRAM during inference
Model Medium: ~1.5 GB VRAM during inference
```

**System Budget**:
```
Total VRAM: 8 GB
ZED Depth:  1.5 GB
Tracker:    0.1 GB
System:     0.5 GB
Available:  ~6 GB for YOLO ‚Üí Nano/Small fit comfortably
```

**Recommendation**: Use **nano** (leaves 5.5 GB headroom).

---

### Pretrained Weights

**Options**:
1. COCO pretrained (default) - ‚úÖ RECOMMENDED
2. From scratch
3. Custom weights...

#### Option 1: COCO Pretrained (Default)

**What it is**: Model pre-trained on COCO dataset (80 object classes, 200k images)

**COCO Dataset**:
- 80 classes: person, car, dog, airplane, etc.
- 200,000+ images
- General object detection knowledge
- Learned features: edges, shapes, textures

**Transfer Learning Process**:
1. Start with COCO-pretrained weights
2. **Fine-tune** on your drone tracking data (2 classes)
3. Model adapts COCO features to your specific task
4. Retains general object detection knowledge

**Advantages**:
- ‚úÖ **Faster convergence**: 50 epochs vs. 150+ from scratch
- ‚úÖ **Better accuracy**: Especially with small datasets (<500 images)
- ‚úÖ **Learned features**: Edges, shapes already understood
- ‚úÖ **Reduced overfitting**: Pre-trained knowledge acts as regularization
- ‚úÖ **Industry standard**: 95% of projects use pretrained weights

**Disadvantages**:
- ‚ö†Ô∏è May retain COCO biases (rare issue)
- ‚ö†Ô∏è Slightly larger initial model (downloads ~20-50 MB)

**When to use**:
- ‚úÖ **Always** (unless specific reason not to)
- ‚úÖ Dataset < 2000 images
- ‚úÖ Want faster training
- ‚úÖ First experiment/baseline

#### Option 2: From Scratch

**What it is**: Random initialization, no pretrained knowledge

**Process**:
1. Start with random weights
2. Learn everything from your data
3. Requires 150-300 epochs (3-6x longer)

**Advantages**:
- ‚úÖ No COCO bias (pure learning)
- ‚úÖ Potentially better if dataset is VERY different from COCO
- ‚úÖ Research/academic interest (ablation study)

**Disadvantages**:
- ‚ùå **Much slower convergence**: 150-300 epochs needed
- ‚ùå **Worse accuracy**: Especially with small datasets
- ‚ùå **Requires more data**: 2000+ images minimum
- ‚ùå **Higher overfitting risk**: No pretrained regularization

**When to use**:
- ‚ùå Almost never for production
- ‚ö†Ô∏è Research question: "How much does pretraining help?"
- ‚ö†Ô∏è Dataset very different from COCO (rare for vision)
- ‚ö†Ô∏è Very large dataset (>10k images)

#### Option 3: Custom Weights

**What it is**: Load weights from a different trained model

**Use Cases**:

1. **Resume previous training**:
   - Continue from checkpoint
   - Train longer without restarting

2. **Transfer from similar model**:
   - Another drone tracking model
   - Similar aerial detection task

3. **Domain adaptation**:
   - Model trained on similar data
   - Fine-tune for your specific conditions

**Requirements**:
- Must be same model architecture (YOLOv8n ‚Üí YOLOv8n only)
- `.pt` file format (PyTorch weights)

**When to use**:
- ‚úÖ Resume interrupted training
- ‚úÖ Fine-tune existing drone model
- ‚úÖ Transfer from simulator to real-world

#### Comparison Table

| Pretrained Option | Training Time | Accuracy (Small Data) | Accuracy (Large Data) | Use Case                    |
|-------------------|---------------|------------------------|------------------------|-----------------------------|
| **COCO (default)**| 50 epochs     | High                   | High                   | ‚úÖ **Always start here**    |
| From Scratch      | 150+ epochs   | Low                    | Medium-High            | Research only               |
| Custom Weights    | 30-50 epochs  | High (if related)      | High (if related)      | Resume or transfer learning |

**Recommendation for Jetson Deployment**:
- **Use COCO pretrained** (option 1) for initial training
- **Why**: Faster convergence, better accuracy with limited data
- **Jetson-specific consideration**: Faster training = less time, lower power consumption

---

### Resume from Checkpoint

**Checkbox**: Resume from checkpoint  
**Path**: Path to `.pt` checkpoint file

#### What It Does

**Resume Training**: Continue training from a saved checkpoint instead of starting from epoch 0.

**Use Cases**:

1. **Training interrupted** (power loss, crash, manual stop)
2. **Extend training** (add more epochs to existing model)
3. **Fine-tune** (small adjustments after initial training)

#### How It Works

**Normal Training**:
```
Epoch 0 ‚Üí 1 ‚Üí 2 ‚Üí ... ‚Üí 100 (start from scratch)
```

**Resume Training**:
```
Previous run: Epoch 0 ‚Üí 50 (stopped)
Resume:       Epoch 51 ‚Üí 100 (continue from 50)
```

**What's Restored**:
- ‚úÖ Model weights (learned parameters)
- ‚úÖ Optimizer state (momentum, learning rate history)
- ‚úÖ Current epoch number
- ‚úÖ Training losses

#### Checkpoint Files

**Location**: Automatically saved during training:
```
runs/yolo_training/run_001/weights/
‚îú‚îÄ‚îÄ best.pt      # Best validation mAP checkpoint
‚îú‚îÄ‚îÄ last.pt      # Latest epoch checkpoint
‚îî‚îÄ‚îÄ epoch50.pt   # Periodic checkpoints (if save_period > 0)
```

**Which to use**:
- **`last.pt`**: Resume from exact point (recommended)
- **`best.pt`**: Resume from best validation performance
- **`epochN.pt`**: Resume from specific epoch

#### Implications

**Advantages**:
- ‚úÖ Don't lose progress if training stops
- ‚úÖ Save time (no re-training)
- ‚úÖ Incremental improvement (add 50 more epochs)
- ‚úÖ Experiment with different later-stage settings

**Considerations**:
- ‚ö†Ô∏è Must use SAME model architecture
- ‚ö†Ô∏è Must use SAME dataset (or compatible)
- ‚ö†Ô∏è Learning rate continues from saved state

**When to use**:
- ‚úÖ Training was interrupted
- ‚úÖ Want more epochs after initial run
- ‚úÖ Fine-tuning after baseline training
- ‚ùå Changing model architecture (must start fresh)
- ‚ùå Completely different dataset (start fresh)

**Best Practices**:
1. Always save checkpoints (automatic in our GUI)
2. Keep multiple checkpoints (last.pt + best.pt)
3. Test resumed model before long training
4. Document resumed runs in logs

---

## Training Parameters

### Image Size

**Options**: 416, 512, 640, 1280  
**Default**: 640

#### What It Means

**Image Size**: Square dimension that images are resized to during training/inference.

**Example**:
```
Original image: 1280x720 (HD720)
Image size 640: Resized to 640x640 (maintains aspect ratio via padding)
```

#### Size vs. Performance Trade-off

| Size | Jetson FPS | Accuracy | Memory | Latency | Use Case                    |
|------|------------|----------|--------|---------|------------------------------|
| 416  | 35-40      | Medium   | Low    | ~20ms   | ‚úÖ **Real-time tracking**   |
| 512  | 25-30      | Good     | Medium | ~30ms   | Balanced performance        |
| 640  | 15-20      | High     | High   | ~50ms   | ‚ùå Too slow for live       |
| 1280 | 4-6        | Highest  | V.High | ~200ms  | ‚ùå Offline processing only |

#### Detailed Analysis

**416x416** - ‚úÖ RECOMMENDED for Jetson
- **Speed**: 35-40 FPS (excellent)
- **Accuracy**: Sufficient for targets >5m
- **Memory**: ~400 MB VRAM
- **Latency**: 20ms (fast response)
- **Use if**: Real-time tracking, close-range (<30m)

**512x512** - ‚ö†Ô∏è ALTERNATIVE
- **Speed**: 25-30 FPS (acceptable)
- **Accuracy**: Better for distant targets (>30m)
- **Memory**: ~500 MB VRAM
- **Latency**: 30ms (acceptable)
- **Use if**: Need extra accuracy for long-range

**640x640** - ‚ùå TOO SLOW
- **Speed**: 15-20 FPS (insufficient)
- **Use if**: Offline analysis only

**1280x1280** - ‚ùå NOT VIABLE
- **Speed**: <10 FPS (unusable)
- **Use if**: Desktop processing with high-end GPU

#### Resolution Impact on Detection

**Small Objects** (target <50 pixels in original image):
- 416: May miss very small targets
- 512: Better detection
- 640: Best detection

**Your Drone Case**:
- **Close range (5-20m)**: Target is large ‚Üí 416 is fine
- **Long range (30-40m)**: Target is small ‚Üí 512 recommended

**Rule of Thumb**:
```
Target size in image: 50+ pixels ‚Üí Use 416
Target size in image: 20-50 pixels ‚Üí Use 512
Target size in image: <20 pixels ‚Üí Use 640 (but too slow for Jetson)
```

#### Training vs. Inference Size

**Can they differ?**
- ‚ö†Ô∏è **No**: Must use same size for training and deployment
- Training @640, deploy @416 ‚Üí Poor accuracy
- Training @416, deploy @416 ‚Üí Good accuracy

**Why**: Model learns features at specific scale.

**Best Practice**:
- Train at the size you'll deploy (416 or 512)
- **Never train at 640 if deploying at 416**

#### Recommendation for Jetson

**High-Speed Chase (Your Case)**:
- **Use 416x416**
- **Why**:
  - 35-40 FPS gives headroom for depth + trajectory
  - Targets are close enough (5-30m) for good detection
  - Low latency critical for high-speed maneuvers

**Alternative (Long-Range Emphasis)**:
- **Use 512x512**
- **If**: Tracking at 30-40m distance is critical
- **Trade-off**: 25-30 FPS (tighter budget)

---

### Batch Size

**Input**: 1-128 (or -1 for auto)  
**Default**: 16

#### What It Is

**Batch Size**: Number of images processed together in one training iteration.

**Training Loop**:
```
For each epoch:
  For each batch of N images:
    1. Forward pass (predict)
    2. Calculate loss
    3. Backward pass (gradients)
    4. Update weights
```

**Example** (batch size = 16):
- 1000 training images
- Batch size 16
- Iterations per epoch: 1000 / 16 = 62.5 ‚âà 63 batches

#### Batch Size Effects

**Small Batch (1-8)**:
- ‚úÖ Less GPU memory
- ‚úÖ More weight updates per epoch
- ‚úÖ Can use larger images
- ‚ùå Slower training (inefficient GPU use)
- ‚ùå Noisier gradients (less stable)
- ‚ùå May hurt accuracy

**Medium Batch (16-32)** - ‚úÖ RECOMMENDED:
- ‚úÖ Good balance
- ‚úÖ Efficient GPU utilization
- ‚úÖ Stable gradients
- ‚úÖ Industry standard
- ‚ö†Ô∏è Requires 4-6 GB VRAM

**Large Batch (64-128)**:
- ‚úÖ Faster training (fewer iterations)
- ‚úÖ Very stable gradients
- ‚ùå High GPU memory (8+ GB)
- ‚ùå May reduce generalization
- ‚ùå Requires careful learning rate tuning

#### GPU Memory vs. Batch Size

**Jetson Orin Nano Super (8GB)**:

| Batch Size | Image Size | VRAM Usage | Feasible? |
|------------|------------|------------|-----------|
| 8          | 416        | ~2.5 GB    | ‚úÖ Yes    |
| 16         | 416        | ~4.0 GB    | ‚úÖ Yes    |
| 32         | 416        | ~7.0 GB    | ‚ö†Ô∏è Tight  |
| 16         | 512        | ~5.5 GB    | ‚úÖ Yes    |
| 32         | 512        | ~10 GB     | ‚ùå OOM    |
| 16         | 640        | ~8.0 GB    | ‚ö†Ô∏è Tight  |

**OOM = Out of Memory**

#### Auto-Batch Detection

**Set batch size = -1**:
- System automatically finds largest batch that fits VRAM
- Tests progressively: 128 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 8
- Stops when training runs without OOM

**When to use**:
- ‚úÖ First training run (find optimal size)
- ‚úÖ Unknown GPU memory availability
- ‚ùå Reproducibility critical (use fixed batch)

#### Batch Size Best Practices

**General Rules**:
1. **Start with 16** (default, works for most cases)
2. **If OOM error**: Reduce to 8 or increase image size
3. **If very slow**: Try 32 (if memory allows)
4. **Never use batch=1** (very unstable, slow)

**Jetson-Specific**:
- **416x416**: Use batch=16 (comfortable fit)
- **512x512**: Use batch=8-16 (tight but manageable)
- **640x640**: Use batch=8 (tight)

**Recommendation for Your Case**:
- **Use batch=16** with 416x416 images
- **Why**: Optimal balance, fits comfortably in 8GB VRAM

---

### Epochs

**Input**: 1-1000  
**Default**: 100

#### What It Is

**Epoch**: One complete pass through the entire training dataset.

**Example** (1000 images, batch=16):
```
Epoch 1: Process all 1000 images (63 batches)
Epoch 2: Process all 1000 images again (63 batches)
...
Epoch 100: Process all 1000 images (63 batches)
```

**Total iterations**: Epochs √ó (Images / Batch Size)  
100 epochs √ó 63 batches = 6,300 iterations

#### How Many Epochs?

**Depends on**:
1. **Pretrained weights**: COCO pretrained needs fewer
2. **Dataset size**: Smaller dataset needs more
3. **Model variant**: Larger models need more
4. **Early stopping**: May stop before reaching max

**General Guidelines**:

| Configuration              | Recommended Epochs | Reason                          |
|----------------------------|--------------------|---------------------------------|
| COCO pretrained, <500 imgs | 50-80              | Fast convergence, small data    |
| COCO pretrained, 500-2k    | 80-120             | Standard case                   |
| COCO pretrained, >2k       | 100-150            | Large dataset, more learning    |
| From scratch, any size     | 200-300            | No pretrained knowledge         |
| YOLOv8 (vs v5)             | -20 to -30%        | v8 converges faster             |

**Your Case** (Drone, COCO pretrained, ~1000 images):
- **Recommended**: 80-100 epochs
- **Why**: COCO pretrained + medium dataset

#### Training Progress

**Typical Learning Curve** (COCO pretrained):
```
Epochs 1-20:   Rapid improvement (mAP 20% ‚Üí 40%)
Epochs 21-50:  Steady improvement (mAP 40% ‚Üí 48%)
Epochs 51-80:  Slow improvement (mAP 48% ‚Üí 52%)
Epochs 81-100: Marginal gains (mAP 52% ‚Üí 53%)
Epochs 100+:   Plateau or overfit (mAP ~53%)
```

**Diminishing Returns**: Later epochs give less improvement.

#### Early Stopping

**Feature**: Automatically stop if no validation improvement for N epochs.

**Example** (patience=50):
```
Epoch 60: Best mAP = 52.5%
Epoch 61-110: No improvement (validation mAP ‚â§ 52.5%)
Epoch 110: Training stops early (patience exceeded)
```

**Benefits**:
- ‚úÖ Saves time (no wasted epochs)
- ‚úÖ Prevents overfitting
- ‚úÖ Automatic (no manual monitoring)

**Recommendation**:
- Set epochs=100, patience=50
- Training will stop ~80-90 if converged

#### Training Time Estimates

**Jetson Orin Nano Super (YOLOv8n, 416, batch=16, 1000 images)**:

| Epochs | Time per Epoch | Total Time |
|--------|----------------|------------|
| 50     | ~3-4 min       | ~2.5-3 hrs |
| 80     | ~3-4 min       | ~4-5 hrs   |
| 100    | ~3-4 min       | ~5-6 hrs   |
| 150    | ~3-4 min       | ~7.5-9 hrs |

**Desktop GPU (RTX 3080)**:
- ~1 min/epoch ‚Üí 100 epochs in ~1.5 hrs

**Note**: Jetson is 3-4x slower than desktop GPU (acceptable for occasional retraining).

**Recommendation**:
- **Use 100 epochs** with early stopping (patience=50)
- **Expected**: Training stops around epoch 80-90
- **Time**: ~5 hours (can run overnight)

---

### Learning Rate

**Input**: 0.0001 - 1.0  
**Default**: 0.01

#### What It Is

**Learning Rate (LR)**: Step size for weight updates during training.

**Analogy**: Hiking down a mountain to find the lowest valley (minimum loss):
- High LR = Large steps (fast but may overshoot valley)
- Low LR = Small steps (slow but precise)

**Weight Update**:
```
New Weight = Old Weight - (Learning Rate √ó Gradient)
```

#### LR Impact on Training

**Too High (LR = 0.1)**:
- ‚ùå Training unstable (loss jumps around)
- ‚ùå May diverge (loss increases instead of decreases)
- ‚ùå Overshoots optimal weights

**Too Low (LR = 0.0001)**:
- ‚ùå Training very slow
- ‚ùå May get stuck in local minima
- ‚ùå Needs many more epochs

**Just Right (LR = 0.01)** - ‚úÖ DEFAULT:
- ‚úÖ Steady convergence
- ‚úÖ Reaches good solution
- ‚úÖ Balanced speed vs. stability

#### LR Tuning Guidelines

**Model Variant**:
- YOLOv8n/YOLOv8s: LR = 0.01 (default)
- YOLOv8m/YOLOv8l: LR = 0.005-0.01 (larger models need lower LR)

**Batch Size**:
- Batch 8: LR = 0.008
- Batch 16: LR = 0.01 (default)
- Batch 32: LR = 0.012-0.015
- **Rule**: LR scales with ‚àö(batch size)

**Dataset Size**:
- Small (<500): LR = 0.01 (default)
- Medium (500-2k): LR = 0.01
- Large (>2k): LR = 0.01-0.015

**Fine-Tuning** (resume training):
- Reduce LR by 10x: 0.01 ‚Üí 0.001
- **Why**: Model is already trained, need smaller adjustments

#### LR Schedulers

**Purpose**: Automatically adjust LR during training.

**Cosine Scheduler** (Default) - ‚úÖ RECOMMENDED:
```
Epoch 1-20:  LR = 0.01 (high, rapid learning)
Epoch 21-50: LR = 0.007 (moderate)
Epoch 51-80: LR = 0.003 (low, fine-tuning)
Epoch 81-100: LR = 0.001 (very low, refinement)
```
- ‚úÖ Smooth decay
- ‚úÖ Works well for most cases
- ‚úÖ No hyperparameter tuning needed

**Linear Scheduler**:
```
LR decreases linearly: 0.01 ‚Üí 0.001
```
- Less common, simpler

**Step Scheduler**:
```
LR drops at fixed intervals:
Epochs 1-30: LR = 0.01
Epochs 31-60: LR = 0.003
Epochs 61-100: LR = 0.001
```
- Requires tuning step points

**None** (Constant LR):
```
LR = 0.01 for all epochs
```
- ‚ùå Not recommended (no refinement phase)

**Recommendation**:
- **Use cosine scheduler** (default)
- **Why**: Automatic, proven, no tuning needed

---

### Optimizer

**Options**: Adam, SGD, AdamW  
**Default**: Adam

#### What It Is

**Optimizer**: Algorithm that updates model weights based on gradients.

**Role**:
- Takes gradients from backpropagation
- Computes weight updates
- Applies updates to model

#### Optimizer Comparison

**Adam** (Adaptive Moment Estimation) - ‚úÖ RECOMMENDED:
- ‚úÖ Fast convergence
- ‚úÖ Works well out-of-the-box
- ‚úÖ Adaptive learning rates per parameter
- ‚úÖ Handles sparse gradients well
- ‚ö†Ô∏è May generalize slightly worse than SGD

**Use Cases**:
- ‚úÖ Default choice (90% of projects)
- ‚úÖ Fast experimentation
- ‚úÖ Limited tuning time
- ‚úÖ **Your case**: Quick training on Jetson

**SGD** (Stochastic Gradient Descent):
- ‚úÖ Better generalization (slightly higher accuracy)
- ‚úÖ Simpler, more interpretable
- ‚ùå Slower convergence (needs more epochs)
- ‚ùå Requires careful LR tuning
- ‚ùå Needs momentum (default: 0.937)

**Use Cases**:
- ‚úÖ Final production model (squeeze last 1-2% accuracy)
- ‚úÖ Research/academic (reproducibility)
- ‚ùå First experiments (too slow)

**AdamW** (Adam with Weight Decay):
- ‚úÖ Better regularization than Adam
- ‚úÖ Slightly better generalization
- ‚úÖ Similar convergence to Adam
- ‚ö†Ô∏è Newer, less widely tested

**Use Cases**:
- ‚úÖ Alternative to Adam (try if Adam overfits)
- ‚úÖ Large models (better regularization)

#### Detailed Comparison

| Feature              | Adam            | SGD             | AdamW           |
|----------------------|-----------------|-----------------|-----------------|
| **Convergence**      | Fast (80 ep)    | Slow (120+ ep)  | Fast (80 ep)    |
| **Ease of Use**      | Easy            | Medium          | Easy            |
| **Generalization**   | Good            | Best            | Good-Best       |
| **LR Sensitivity**   | Low             | High            | Low             |
| **Memory**           | +20% overhead   | Minimal         | +20% overhead   |
| **Hyperparameters**  | Few             | Many            | Few             |

#### Hyperparameters

**Adam**:
- Learning Rate: 0.01 (default)
- Beta1: 0.9 (momentum for gradients)
- Beta2: 0.999 (momentum for squared gradients)
- **Usually don't need to tune**

**SGD**:
- Learning Rate: 0.01 (critical to tune)
- Momentum: 0.937 (default, range: 0.9-0.99)
- Weight Decay: 0.0005 (regularization)
- **Requires careful tuning**

**AdamW**:
- Learning Rate: 0.01
- Weight Decay: 0.05 (stronger than SGD)
- **Less tuning than SGD**

#### Recommendation

**For Jetson Training**:
1. **Start with Adam** (default)
   - Fast convergence
   - Minimal tuning
   - Reliable results

2. **Try AdamW** if:
   - Adam overfits (high train/val gap)
   - Want slightly better generalization

3. **Use SGD** only if:
   - Retraining final production model
   - Have time for 150+ epochs
   - Want absolute best accuracy

**Your Case**:
- **Use Adam** (default)
- **Why**: Fast training on Jetson, minimal tuning

---

## Augmentation Settings

Data augmentation artificially increases dataset diversity by applying random transformations during training.

### Augmentation Presets

**Options**: none, light, moderate, heavy  
**Default**: moderate

#### Purpose

**Problem**: Limited training data ‚Üí Model overfits ‚Üí Poor generalization  
**Solution**: Augmentation creates virtual training examples ‚Üí Model sees more variety

#### Preset Comparison

**None** - ‚ùå NOT RECOMMENDED:
- No augmentation
- Use raw images only
- **Risk**: Severe overfitting

**Light**:
- Minimal transformations
- Horizontal flip (50%)
- Slight color jitter
- Small scaling (¬±25%)
- **Use if**: Very clean data, concerned about artifacts

**Moderate** - ‚úÖ RECOMMENDED:
- Balanced augmentation
- Horizontal flip (50%)
- Color adjustments (HSV)
- Scaling (¬±50%)
- Translation (¬±10%)
- Mosaic (100%) - combine 4 images
- **Use if**: Standard case (YOUR CASE)

**Heavy**:
- Aggressive transformations
- Rotation (¬±10¬∞)
- Strong color jitter
- Large scaling (¬±90%)
- Shear, perspective transforms
- Mixup (10%) - blend two images
- **Use if**: Very small dataset (<300 images)

#### Visual Examples

**Horizontal Flip**:
```
Original: Target moving left
Flipped:  Target moving right
‚Üí Model learns direction-invariance
```

**Color (HSV) Jitter**:
```
Original: Normal daylight
Augmented: Darker (clouds), brighter (sun), color shift
‚Üí Model learns lighting-invariance
```

**Scaling**:
```
Original: Target at 50 pixels
Scaled:   Target at 25-75 pixels
‚Üí Model learns size-invariance
```

**Mosaic** (4 images combined):
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Img1‚îÇ Img2‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Img3‚îÇ Img4‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚Üí Model learns occlusion, multiple scales
```

#### When to Use Each Preset

| Dataset Size | Diversity       | Preset          | Reason                          |
|--------------|-----------------|-----------------|----------------------------------|
| <300 images  | Low             | Heavy           | Need maximum variation          |
| 300-1000     | Medium          | Moderate        | Balanced augmentation           |
| >1000        | High            | Light-Moderate  | Data already diverse            |
| Any          | Very uniform    | Heavy           | e.g., all same location/weather |
| Any          | Very diverse    | Light           | e.g., many locations/conditions |

**Your Case** (Drone tracking, ~1000 images):
- **Use Moderate** (default)
- **Why**: Standard dataset size, expect varied conditions

---

### Individual Augmentation Parameters

**Note**: These override preset values if manually adjusted.

#### Horizontal Flip Probability

**Range**: 0.0 - 1.0  
**Default**: 0.5 (50% chance)

**What it does**: Randomly flips images left-right.

**Use Cases**:
- ‚úÖ Objects can appear from any direction
- ‚úÖ Left/right symmetry doesn't matter
- ‚úÖ **Your case**: Drone can approach from left or right

**Don't use if** (set to 0.0):
- ‚ùå Direction matters (e.g., reading text)
- ‚ùå Asymmetric objects (rare in nature)

**Recommendation**: 0.5 (default)

---

#### Mosaic Probability

**Range**: 0.0 - 1.0  
**Default**: 1.0 (100% of batches)

**What it does**: Combines 4 images into one, creating complex scenes.

**Benefits**:
- ‚úÖ Simulates occlusion
- ‚úÖ Exposes model to multiple scales simultaneously
- ‚úÖ Improves small object detection
- ‚úÖ Reduces overfitting

**Downsides**:
- ‚ö†Ô∏è Creates artificial boundaries (may confuse model slightly)

**Recommendation**:
- 1.0 for training epochs 1-80
- 0.0 for last 20 epochs (fine-tuning on real images)
- **YOLO automatically does this**

---

#### Scale Augmentation

**Range**: 0.0 - 1.0 (¬±gain)  
**Default**: 0.5 (¬±50% size variation)

**What it does**: Randomly scales images (zoom in/out).

**Example** (scale=0.5):
- 50% chance: Zoom out (-50% size)
- 50% chance: Zoom in (+50% size)

**Use Cases**:
- ‚úÖ Target distance varies (5m to 40m in your case)
- ‚úÖ Improve scale invariance

**Your Case**:
- Target size varies greatly (close vs far)
- **Recommendation**: 0.5 (default)

---

#### Translate Augmentation

**Range**: 0.0 - 0.5 (¬±fraction of image)  
**Default**: 0.1 (¬±10%)

**What it does**: Randomly shifts images left/right/up/down.

**Example** (translate=0.1, 640px image):
- Shift up to ¬±64 pixels in any direction

**Use Cases**:
- ‚úÖ Target can appear anywhere in frame
- ‚úÖ Compensates for imperfect centering

**Your Case**:
- Drone may not perfectly center target
- **Recommendation**: 0.1 (default)

---

### Augmentation Best Practices

1. **Start with preset** (moderate)
2. **Train baseline model**
3. **Evaluate overfitting**:
   - If train accuracy >> val accuracy ‚Üí Increase augmentation (heavy)
   - If both low ‚Üí Decrease augmentation (light) or improve data quality

4. **A/B test**:
   - Moderate vs. Heavy
   - Evaluate on test set

**Your Case**:
- **Use moderate preset**
- **Don't modify** individual parameters initially
- **Adjust only if** baseline model overfits

---

## Performance vs. Accuracy Trade-offs

### Summary Table

| Configuration           | Jetson FPS | mAP50 | Use Case                    |
|-------------------------|------------|-------|------------------------------|
| **YOLOv8n @ 416**       | 35-40      | 52%   | ‚úÖ **High-speed chase**     |
| YOLOv8n @ 512           | 25-30      | 54%   | Long-range emphasis         |
| YOLOv8s @ 416           | 20-25      | 62%   | ‚ö†Ô∏è Fallback (if nano fails) |
| YOLOv8n @ 640           | 15-20      | 55%   | ‚ùå Too slow                 |
| YOLOv8s @ 512           | 12-18      | 64%   | ‚ùå Too slow                 |

### Accuracy Improvement Strategies

If YOLOv8n @ 416 doesn't meet accuracy requirements:

**Option 1: Increase resolution** ‚Üí YOLOv8n @ 512
- +2-3% mAP
- -30% FPS (25-30 FPS, still acceptable)

**Option 2: Increase model size** ‚Üí YOLOv8s @ 416
- +10% mAP
- -40% FPS (20-25 FPS, tight but usable)

**Option 3: Better training** (before hardware upgrade):
- Collect more data (especially negatives)
- Use heavy augmentation
- Train longer (150 epochs)
- ‚Üí Can gain 3-5% mAP without hardware change

**Option 4: TensorRT optimization**:
- FP16: 1.5-2x faster
- INT8: 2-3x faster (slight accuracy loss <1%)
- ‚Üí Can use YOLOv8s @ 416 at 30-40 FPS

---

## Real-World Deployment Considerations

### Training-to-Deployment Checklist

**Phase 1: Training (Desktop/Jetson)**
- ‚úÖ Use same image size as deployment
- ‚úÖ Train with COCO pretrained weights
- ‚úÖ Use moderate augmentation
- ‚úÖ Validate on diverse test set

**Phase 2: Export**
- ‚úÖ Convert to TensorRT FP16 or INT8
- ‚úÖ Test inference speed on Jetson
- ‚úÖ Verify accuracy after conversion

**Phase 3: Integration**
- ‚úÖ Optimize depth processing pipeline
- ‚úÖ Integrate CSRT tracker
- ‚úÖ Profile end-to-end latency
- ‚úÖ Stress test under thermal load

**Phase 4: Field Testing**
- ‚úÖ Test various lighting conditions
- ‚úÖ Test various distances (5-40m)
- ‚úÖ Test high-speed maneuvers
- ‚úÖ Monitor false positives/negatives

### Jetson-Specific Optimizations

**1. TensorRT Conversion**:
```bash
yolo export model=best.pt format=engine half=True
```
- 2x faster inference
- Minimal accuracy loss

**2. Power Mode**:
```bash
sudo nvpmodel -m 0  # Max performance (15W)
```

**3. Thermal Management**:
- Active cooling recommended for sustained use
- Monitor GPU temperature (`tegrastats`)

**4. Memory Management**:
- Close unused processes
- Monitor VRAM usage
- Ensure swap is disabled (latency)

### Expected Real-World Performance

**YOLOv8n @ 416 + TensorRT FP16 + Jetson Orin Nano Super**:
- **Detection**: 50-60 FPS
- **Depth Processing**: 20-30 FPS (ZED SDK)
- **Combined Pipeline**: 17-25 FPS
- **Flight Controller Output**: 5-10 Hz ‚úÖ

**Bottleneck**: Depth processing (not YOLO)  
**Solution**: Optimize depth extraction (ROI-only, reduce resolution)

---

## Conclusion

### Final Recommendations for Jetson Orin Nano Super

**Model Configuration**:
- **Model**: YOLOv8n
- **Image Size**: 416x416
- **Batch Size**: 16
- **Epochs**: 100 (with early stopping)
- **Learning Rate**: 0.01
- **Optimizer**: Adam
- **Augmentation**: Moderate
- **Pretrained**: COCO

**Expected Results**:
- **Accuracy**: 50-54% mAP50
- **Speed**: 50-60 FPS (TensorRT)
- **Latency**: ~16-20ms per frame
- **Power**: ~8-10W sustained

**Training Time**:
- ~5-6 hours on Jetson
- ~1-2 hours on desktop GPU (recommended for faster iteration)

**Deployment**:
- Export to TensorRT FP16
- Integrate with ZED depth + CSRT tracker
- Target 5-10 Hz flight controller updates ‚úÖ

---

**Document Version**: 1.0  
**Last Updated**: December 3, 2025  
**Maintainer**: SVO2-Handler Development Team  
**Contact**: See README.md

---

## Appendix: Glossary

- **mAP50**: Mean Average Precision at IoU 0.5 (50% overlap threshold)
- **FPS**: Frames Per Second
- **VRAM**: Video RAM (GPU memory)
- **TensorRT**: NVIDIA's inference optimizer
- **FP16**: 16-bit floating point (half precision)
- **INT8**: 8-bit integer quantization
- **IoU**: Intersection over Union (overlap metric)
- **Epoch**: One complete pass through training data
- **Batch**: Group of images processed together
- **Learning Rate**: Step size for weight updates
- **Overfitting**: Model memorizes training data, poor on new data
- **Generalization**: Model performs well on unseen data
- **Transfer Learning**: Starting from pretrained weights
- **Fine-Tuning**: Adapting pretrained model to new task
